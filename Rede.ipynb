{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'sdss.csv'\n",
    "raw_data = []\n",
    "with open(filename) as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        #print(row.split(','))\n",
    "        raw_data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "x_data = []\n",
    "y_label = []\n",
    "for row in raw_data[1:]:\n",
    "    y_label.append(row[13])   # Append label\n",
    "    x_data.append(list(map(float, (row[0:13] + row[14:])))) # Convert to list of float\n",
    "x_data = np.array(x_data)\n",
    "print(len(x_data))\n",
    "print(len(y_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_encoding = preprocessing.LabelBinarizer()\n",
    "lb_encoding.fit(y_label)\n",
    "y_data = lb_encoding.transform(y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['GALAXY', 'QSO', 'STAR'], dtype='<U6')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb_encoding.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['GALAXY', 'QSO', 'STAR'], dtype='<U6')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb_encoding.inverse_transform(np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for useless data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Useless feature in index  0\n",
      "Useless feature in index  9\n"
     ]
    }
   ],
   "source": [
    "std_list = [np.std(x_data[:,i]) for i in range(x_data.shape[1])]\n",
    "\n",
    "for i,std in enumerate(std_list):\n",
    "    if std == 0:\n",
    "        print(\"Useless feature in index \",i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Here we found two features that are exactly the same for all samples. Therefore, they are useless and are going to be removed. Their indices are 0 and 9 </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_features = [i for i in range(x_data.shape[1]) if i not in [0,9]]\n",
    "x_data = x_data[:,valid_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 15)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Subtracting by mean and dividing by standand deviation\n",
    "def normalize_data(x_data):\n",
    "    for i in range(x_data.shape[1]):\n",
    "        x_data[:, i] = x_data[:, i] - np.mean(x_data[:, i])\n",
    "        x_data[:, i] = x_data[:, i]/np.std(x_data[:, i])\n",
    "    return x_data\n",
    "\n",
    "x_data = normalize_data(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.684341886080802e-16 1.0\n",
      "-6.892264536872972e-17 0.9999999999999999\n",
      "-1.2509104863056565e-15 1.0\n",
      "8.874678769643651e-16 1.0\n",
      "-3.588240815588506e-17 0.9999999999999998\n",
      "-3.872457909892546e-16 1.0\n",
      "1.6470380614919123e-15 0.9999999999999999\n",
      "-5.4001247917767614e-17 1.0\n",
      "-3.552713678800501e-17 0.9999999999999999\n",
      "6.536993168992922e-17 1.0\n",
      "1.1795009413617663e-16 0.9999999999999999\n",
      "6.536993168992922e-17 1.0\n",
      "-4.9737991503207014e-17 1.0\n",
      "-2.1593393739749445e-15 0.9999999999999998\n",
      "4.263256414560601e-17 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(x_data.shape[1]):\n",
    "    print(np.mean(x_data[:,i]), np.std(x_data[:,i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split  in Train, Validation and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shuffle data \n",
    "def shuffle_lists_the_same_way(x_data, y_data):\n",
    "    assert len(x_data) == len(y_data)\n",
    "    p = np.random.permutation(len(x_data))\n",
    "    return np.array(x_data)[p], np.array(y_data)[p]\n",
    "\n",
    "x_data, y_data = shuffle_lists_the_same_way(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separating 70% train and 30% test\n",
    "train_percentage = 0.7\n",
    "validation_percentage = 0\n",
    "test_percentage = 0.3\n",
    "\n",
    "def separare_train_validation_test(x_data, y_data, train_percentage=0.6, validation_percentage=0.2, test_percentage=0.2):\n",
    "    total = len(x_data)\n",
    "    train_slice = int(train_percentage*total)\n",
    "    validation_slice = train_slice + int(validation_percentage*total)\n",
    "    test_slice = validation_slice + int(test_percentage*total)\n",
    "    \n",
    "    x_train, y_train = x_data[:train_slice], y_data[:train_slice]\n",
    "    x_validation, y_validation = x_data[train_slice:validation_slice], y_data[train_slice:validation_slice]\n",
    "    x_test, y_test = x_data[validation_slice:test_slice], y_data[validation_slice:test_slice]\n",
    "    return x_train, y_train, x_validation, y_validation, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_validation, y_validation, x_test, y_test = separare_train_validation_test(x_data, y_data, train_percentage, validation_percentage, test_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3500, 15) (3500, 3)\n",
      "(0, 15) (0, 3)\n",
      "(1500, 15) (1500, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "print(x_validation.shape, y_validation.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Data were split randomly, checking with number of samples for each class are somewhat distributed </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1736  275 1489]\n",
      "[765 137 598]\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(y_train, axis=0))\n",
    "print(np.sum(y_test, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Testing number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = []\n",
    "test_acc = []\n",
    "response_list = []\n",
    "hidden_nodes_list = [16, 32, 64, 128, 256, 512, 1024, 2048]\n",
    "\n",
    "for hidden_nodes in hidden_nodes_list :\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_nodes, activation='relu', input_shape=([15])))\n",
    "    model.add(Dense(3, activation='relu'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    response = model.fit(x_train, y_train, \n",
    "              batch_size=128, epochs=20, verbose=0)\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    train_acc.append(response.history['acc'][-1])\n",
    "    test_acc.append(score[1])\n",
    "    response_list.append(response.history['acc'])\n",
    "    keras.backend.clear_session()\n",
    "\n",
    "print(\"Train acc: \", train_acc)\n",
    "print(\"Test acc: \", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Testing number of nodes in second hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = []\n",
    "test_acc = []\n",
    "response_list = []\n",
    "hidden_nodes_list = [16, 32, 64, 128, 256, 512, 1024, 2048]\n",
    "\n",
    "for hidden_nodes in hidden_nodes_list :\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_shape=([15])))\n",
    "    model.add(Dense(hidden_nodes, activation='relu'))\n",
    "    model.add(Dense(3, activation='relu'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    response = model.fit(x_train, y_train, \n",
    "              batch_size=128, epochs=20, verbose=0)\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    train_acc.append(response.history['acc'][-1])\n",
    "    test_acc.append(score[1])\n",
    "    response_list.append(response.history['acc'])\n",
    "    keras.backend.clear_session()### 3.1 Testing number of nodes\n",
    "\n",
    "print(\"Train acc: \", train_acc)\n",
    "print(\"Test acc: \", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Testing learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = []\n",
    "test_acc = []\n",
    "response_list = []\n",
    "learning_rates = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05]\n",
    "\n",
    "for learning_rate in learning_rates :\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_shape=([15])))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dense(3, activation='relu'))\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    response = model.fit(x_train, y_train, \n",
    "              batch_size=128, epochs=20, verbose=0)\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    train_acc.append(response.history['acc'][-1])\n",
    "    test_acc.append(score[1])\n",
    "    response_list.append(response.history['acc'])\n",
    "    keras.backend.clear_session()\n",
    "\n",
    "print(\"Train acc: \", train_acc)\n",
    "print(\"Test acc: \", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=([15])))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=0.001, decay=1.5)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "response = model.fit(x_train, y_train, \n",
    "          batch_size=128, epochs=20, verbose=0)\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "train_acc = response.history['acc'][-1]\n",
    "test_acc = score[1]\n",
    "response_list.append(response.history['acc'])\n",
    "keras.backend.clear_session()\n",
    "\n",
    "print(\"Train acc: \", train_acc)\n",
    "print(\"Test acc: \", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Testing batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = []\n",
    "test_acc = []\n",
    "response_list = []\n",
    "batch_sizes = [1, 8, 32, 64, 128, 256, 512]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_shape=([15])))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dense(3, activation='relu'))\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(lr=0.0001)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    response = model.fit(x_train, y_train, \n",
    "              batch_size=batch_size, epochs=20, verbose=0)\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    train_acc.append(response.history['acc'][-1])\n",
    "    test_acc.append(score[1])\n",
    "    response_list.append(response.history['acc'])\n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "print(\"Train acc: \", train_acc)\n",
    "print(\"Test acc: \", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling Quasar class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample(x_data, y_data):\n",
    "    i = 0\n",
    "    x_oversampled = list(x_data)\n",
    "    y_oversampled = list(y_data)\n",
    "    for x,y in zip(x_data, y_data):\n",
    "        if y[1] == 1:\n",
    "            for i in range(5):\n",
    "                x_oversampled.append(x)\n",
    "                y_oversampled.append(y)\n",
    "    return shuffle_lists_the_same_way(x_oversampled, y_oversampled)\n",
    "            \n",
    "x_data_oversampled, y_data_oversampled = oversample(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2501 2472 2087]\n",
      "7060\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(y_data_oversampled, axis=0))\n",
    "print(np.sum(y_data_oversampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc:  [0.9251428602082389, 0.9148571406773158, 0.910571425642286]\n",
      "Test acc:  [0.9060000004768372, 0.8933333334922791, 0.9073333328564962]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "response_list = []\n",
    "for train_index, test_index in skf.split(x_data_oversampled, lb_encoding.inverse_transform(y_data_oversampled)):\n",
    "    x_data_fold = x_data_oversampled[train_index]\n",
    "    y_data_fold = y_data_oversampled[test_index]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_shape=([15])))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dense(3, activation='relu'))\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(lr=0.0001)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    response = model.fit(x_train, y_train, \n",
    "              batch_size=512, epochs=20, verbose=0)\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    train_acc.append(response.history['acc'][-1])\n",
    "    test_acc.append(score[1])\n",
    "    response_list.append(response.history['acc'])\n",
    "    keras.backend.clear_session()\n",
    "\n",
    "print(\"Train acc: \", train_acc)\n",
    "print(\"Test acc: \", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy in oversampled test data: 0.9022222222752041 +- 0.0063089196991281205\n"
     ]
    }
   ],
   "source": [
    "print(\"Final accuracy in oversampled test data: {} +- {}\".format(np.mean(test_acc), np.std(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               8192      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 3075      \n",
      "=================================================================\n",
      "Total params: 536,579\n",
      "Trainable params: 536,579\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
