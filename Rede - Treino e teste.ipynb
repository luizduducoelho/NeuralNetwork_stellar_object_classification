{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'sdss.csv'\n",
    "raw_data = []\n",
    "with open(filename) as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        #print(row.split(','))\n",
    "        raw_data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "x_data = []\n",
    "y_label = []\n",
    "for row in raw_data[1:]:\n",
    "    y_label.append(row[13])   # Append label\n",
    "    x_data.append(list(map(float, (row[0:13] + row[14:])))) # Convert to list of float\n",
    "x_data = np.array(x_data)\n",
    "print(len(x_data))\n",
    "print(len(y_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_encoding = preprocessing.LabelBinarizer()\n",
    "lb_encoding.fit(y_label)\n",
    "y_data = lb_encoding.transform(y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['GALAXY', 'QSO', 'STAR'], dtype='<U6')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb_encoding.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['GALAXY', 'QSO', 'STAR'], dtype='<U6')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb_encoding.inverse_transform(np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for useless data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Useless feature in index  0\n",
      "Useless feature in index  9\n"
     ]
    }
   ],
   "source": [
    "std_list = [np.std(x_data[:,i]) for i in range(x_data.shape[1])]\n",
    "\n",
    "for i,std in enumerate(std_list):\n",
    "    if std == 0:\n",
    "        print(\"Useless feature in index \",i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Here we found two features that are exactly the same for all samples. Therefore, they are useless and are going to be removed. Their indices are 0 and 9 </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_features = [i for i in range(x_data.shape[1]) if i not in [0,9]]\n",
    "x_data = x_data[:,valid_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 15)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Subtracting by mean and dividing by standand deviation\n",
    "def normalize_data(x_data):\n",
    "    for i in range(x_data.shape[1]):\n",
    "        x_data[:, i] = x_data[:, i] - np.mean(x_data[:, i])\n",
    "        x_data[:, i] = x_data[:, i]/np.std(x_data[:, i])\n",
    "    return x_data\n",
    "\n",
    "x_data = normalize_data(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.684341886080802e-16 1.0\n",
      "-6.892264536872972e-17 0.9999999999999999\n",
      "-1.2509104863056565e-15 1.0\n",
      "8.874678769643651e-16 1.0\n",
      "-3.588240815588506e-17 0.9999999999999998\n",
      "-3.872457909892546e-16 1.0\n",
      "1.6470380614919123e-15 0.9999999999999999\n",
      "-5.4001247917767614e-17 1.0\n",
      "-3.552713678800501e-17 0.9999999999999999\n",
      "6.536993168992922e-17 1.0\n",
      "1.1795009413617663e-16 0.9999999999999999\n",
      "6.536993168992922e-17 1.0\n",
      "-4.9737991503207014e-17 1.0\n",
      "-2.1593393739749445e-15 0.9999999999999998\n",
      "4.263256414560601e-17 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(x_data.shape[1]):\n",
    "    print(np.mean(x_data[:,i]), np.std(x_data[:,i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split  in Train, Validation and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shuffle data \n",
    "def shuffle_lists_the_same_way(x_data, y_data):\n",
    "    assert len(x_data) == len(y_data)\n",
    "    p = np.random.permutation(len(x_data))\n",
    "    return np.array(x_data)[p], np.array(y_data)[p]\n",
    "\n",
    "x_data, y_data = shuffle_lists_the_same_way(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separating 70% train and 30% test\n",
    "train_percentage = 0.7\n",
    "validation_percentage = 0\n",
    "test_percentage = 0.3\n",
    "\n",
    "def separare_train_validation_test(x_data, y_data, train_percentage=0.6, validation_percentage=0.2, test_percentage=0.2):\n",
    "    total = len(x_data)\n",
    "    train_slice = int(train_percentage*total)\n",
    "    validation_slice = train_slice + int(validation_percentage*total)\n",
    "    test_slice = validation_slice + int(test_percentage*total)\n",
    "    \n",
    "    x_train, y_train = x_data[:train_slice], y_data[:train_slice]\n",
    "    x_validation, y_validation = x_data[train_slice:validation_slice], y_data[train_slice:validation_slice]\n",
    "    x_test, y_test = x_data[validation_slice:test_slice], y_data[validation_slice:test_slice]\n",
    "    return x_train, y_train, x_validation, y_validation, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_validation, y_validation, x_test, y_test = separare_train_validation_test(x_data, y_data, train_percentage, validation_percentage, test_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3500, 15) (3500, 3)\n",
      "(0, 15) (0, 3)\n",
      "(1500, 15) (1500, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "print(x_validation.shape, y_validation.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Data were split randomly, checking with number of samples for each class are somewhat distributed </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1742  299 1459]\n",
      "[759 113 628]\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(y_train, axis=0))\n",
    "print(np.sum(y_test, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc:  [0.781999999659402, 0.878857142993382, 0.8882857141494751, 0.9065714289120266, 0.8762857142175947, 0.8994285712923322, 0.8597142853736878, 0.944000000340598]\n",
      "Test acc:  [0.7633333328564962, 0.8573333330154419, 0.8659999996821086, 0.8819999996821085, 0.844, 0.8686666666666667, 0.8580000004768371, 0.914666666507721]\n"
     ]
    }
   ],
   "source": [
    "train_acc = []\n",
    "test_acc = []\n",
    "response_list = []\n",
    "hidden_nodes_list = [16, 32, 64, 128, 256, 512, 1024, 2048]\n",
    "\n",
    "for hidden_nodes in hidden_nodes_list :\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_nodes, activation='relu', input_shape=([15])))\n",
    "    model.add(Dense(3, activation='relu'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    response = model.fit(x_train, y_train, \n",
    "              batch_size=128, epochs=20, verbose=0)\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    train_acc.append(response.history['acc'][-1])\n",
    "    test_acc.append(score[1])\n",
    "    response_list.append(response.history['acc'])\n",
    "    keras.backend.clear_session()\n",
    "\n",
    "print(\"Train acc: \", train_acc)\n",
    "print(\"Test acc: \", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc:  [0.880857142652784, 0.6757142856461661, 0.8920000000681196, 0.909714285509927, 0.9271428570066179, 0.9042857143538339, 0.9597142857142857, 0.5700000001362392]\n",
      "Test acc:  [0.8559999996821086, 0.6606666661898295, 0.87, 0.9093333338101705, 0.9053333334922791, 0.8973333330154419, 0.9399999998410543, 0.5693333338101705]\n"
     ]
    }
   ],
   "source": [
    "train_acc = []\n",
    "test_acc = []\n",
    "response_list = []\n",
    "hidden_nodes_list = [16, 32, 64, 128, 256, 512, 1024, 2048]\n",
    "\n",
    "for hidden_nodes in hidden_nodes_list :\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_shape=([15])))\n",
    "    model.add(Dense(hidden_nodes, activation='relu'))\n",
    "    model.add(Dense(3, activation='relu'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    response = model.fit(x_train, y_train, \n",
    "              batch_size=128, epochs=20, verbose=0)\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    train_acc.append(response.history['acc'][-1])\n",
    "    test_acc.append(score[1])\n",
    "    response_list.append(response.history['acc'])\n",
    "    keras.backend.clear_session()\n",
    "\n",
    "print(\"Train acc: \", train_acc)\n",
    "print(\"Test acc: \", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc:  [0.9540000000681196, 0.9225714286395482, 0.8771428574834551, 0.8791428572790964, 0.8891428572109767, 0.7348571425846645]\n",
      "Test acc:  [0.9199999998410543, 0.9120000001589457, 0.8640000004768371, 0.889333333492279, 0.8873333338101705, 0.7493333328564962]\n"
     ]
    }
   ],
   "source": [
    "train_acc = []\n",
    "test_acc = []\n",
    "response_list = []\n",
    "learning_rates = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05]\n",
    "\n",
    "for learning_rate in learning_rates :\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_shape=([15])))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dense(3, activation='relu'))\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    response = model.fit(x_train, y_train, \n",
    "              batch_size=128, epochs=20, verbose=0)\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    train_acc.append(response.history['acc'][-1])\n",
    "    test_acc.append(score[1])\n",
    "    response_list.append(response.history['acc'])\n",
    "    keras.backend.clear_session()\n",
    "\n",
    "print(\"Train acc: \", train_acc)\n",
    "print(\"Test acc: \", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc:  0.5694285716329303\n",
      "Test acc:  0.5680000004768372\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=([15])))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=0.001, decay=1.5)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "response = model.fit(x_train, y_train, \n",
    "          batch_size=128, epochs=20, verbose=0)\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "train_acc = response.history['acc'][-1]\n",
    "test_acc = score[1]\n",
    "response_list.append(response.history['acc'])\n",
    "keras.backend.clear_session()\n",
    "\n",
    "print(\"Train acc: \", train_acc)\n",
    "print(\"Test acc: \", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc:  [0.4605714285714286, 0.9431428571428572, 0.9205714285714286, 0.95, 0.9240000000681196, 0.7928571439470563, 0.9219999998637608]\n",
      "Test acc:  [0.4533333334128062, 0.916666666507721, 0.8793333330154419, 0.9126666668256124, 0.8406666671435038, 0.7913333328564962, 0.8840000001589458]\n"
     ]
    }
   ],
   "source": [
    "train_acc = []\n",
    "test_acc = []\n",
    "response_list = []\n",
    "batch_sizes = [1, 8, 32, 64, 128, 256, 512]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_shape=([15])))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dense(3, activation='relu'))\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(lr=0.0001)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    response = model.fit(x_train, y_train, \n",
    "              batch_size=batch_size, epochs=20, verbose=0)\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    train_acc.append(response.history['acc'][-1])\n",
    "    test_acc.append(score[1])\n",
    "    response_list.append(response.history['acc'])\n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "print(\"Train acc: \", train_acc)\n",
    "print(\"Test acc: \", test_acc)\n",
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final results using oversampling and K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
